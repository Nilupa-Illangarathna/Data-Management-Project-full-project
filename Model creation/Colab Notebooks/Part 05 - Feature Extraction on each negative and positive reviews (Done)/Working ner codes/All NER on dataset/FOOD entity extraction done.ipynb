{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1bd0hY5yQbhAzqTVU7-rvnlzeXdQ0TKbj","authorship_tag":"ABX9TyOFRxDbb7FRAwZV/UWZIU6J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Addresses**"],"metadata":{"id":"ACLGeQQ5GprW"}},{"cell_type":"code","source":["foods_entities_text_file_location = \"/content/drive/MyDrive/Colab Notebooks/Data management Project/Data Management Project - Notebooks/Part 05 - Feature Extraction on each negative and positive reviews/Working ner codes/All NER on dataset/Foods words set.txt\"  # Update this with the correct path"],"metadata":{"id":"iGDowjBYovY3","executionInfo":{"status":"ok","timestamp":1700068956284,"user_tz":-330,"elapsed":13,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Read write on txt functions**"],"metadata":{"id":"bwjJ_lD9Gt1H"}},{"cell_type":"code","source":["def load_strings_from_text_file(text_file_path):\n","    with open(text_file_path, 'r') as file:\n","        lines = file.readlines()\n","        strings_list = [line.strip() for line in lines]\n","    return strings_list\n","\n","def combine_and_save_strings_to_text_file(text_file_path, new_strings_list):\n","    existing_strings_list = load_strings_from_text_file(text_file_path)\n","    combined_set = set(existing_strings_list + new_strings_list)\n","    combined_list = list(combined_set)\n","\n","    with open(text_file_path, 'w') as file:\n","        for string in combined_list:\n","            file.write(string + '\\n')"],"metadata":{"id":"pUW0STAVsI1m","executionInfo":{"status":"ok","timestamp":1700068956284,"user_tz":-330,"elapsed":12,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Path to the text file\n","text_file_path = foods_entities_text_file_location"],"metadata":{"id":"R-IAI9WEGoc6","executionInfo":{"status":"ok","timestamp":1700068956284,"user_tz":-330,"elapsed":11,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Load strings from the text file and print the list\n","strings_list = load_strings_from_text_file(text_file_path)\n","print(\"Strings from the file:\")\n","print(strings_list)\n","print(len(strings_list))\n","\n","\n","# Define a new list of strings to add\n","new_strings_list = [\n","        # Add more entities if yu want to add more into the words txt ///////////////////////////////////////\n","    ]\n","\n","# Combine and save the updated list to the text file\n","combine_and_save_strings_to_text_file(text_file_path, new_strings_list)\n","print(\"\\nCombined list after adding new strings and saving to the file:\")\n","\n","upodated_text = load_strings_from_text_file(text_file_path)\n","print(upodated_text)\n","print(len(upodated_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"jUE9xPZks1PD","executionInfo":{"status":"error","timestamp":1700068956284,"user_tz":-330,"elapsed":11,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}},"outputId":"79008b1f-d7cf-46b0-f8f4-cf20c1bb7030"},"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e1ca86310856>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load strings from the text file and print the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstrings_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_strings_from_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Strings from the file:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-21f5588cd8e2>\u001b[0m in \u001b[0;36mload_strings_from_text_file\u001b[0;34m(text_file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_strings_from_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstrings_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstrings_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Data management Project/Data Management Project - Notebooks/Part 05 - Feature Extraction on each negative and positive reviews/Working ner codes/All NER on dataset/Foods words set.txt'"]}]},{"cell_type":"code","source":["example = \"\"\"\n","\n","In a world of vibrant diversity, travelers embark on journeys that span continents and cultures. From the bustling streets of Tokyo, Japan, to the historic charm of Rome, Italy, each city offers a unique tapestry of experiences. The serene beauty of Sydney, Australia, contrasts with the modern marvels of Dubai, United Arab Emirates. Amidst the enchanting landscapes of Cape Town, South Africa, and the cultural fusion of Buenos Aires, Argentina, wanderers discover the heartbeats of nations. Paris, France, entices with its romantic allure, while New York City, USA, dazzles with its iconic skyline. These global destinations weave tales of exploration and discovery, inviting us to embrace the world's kaleidoscope.\n","\n","\"\"\""],"metadata":{"id":"V9m4S3mLHapG","executionInfo":{"status":"aborted","timestamp":1700068956285,"user_tz":-330,"elapsed":10,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"iQkZWKDsBwU1","executionInfo":{"status":"aborted","timestamp":1700068958052,"user_tz":-330,"elapsed":2,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import pipeline\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n","model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n","\n","nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n","\n","ner_results = nlp(example)\n","for i in range(len(ner_results)):\n","  print(ner_results[i])"],"metadata":{"id":"8iHcS2PFB5WC","executionInfo":{"status":"aborted","timestamp":1700068958053,"user_tz":-330,"elapsed":2,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import pipeline\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","\n","# Sample list of food items from the text file\n","path_to_food_items = foods_entities_text_file_location #####################################################\n","\n","with open(path_to_food_items, \"r\") as file:\n","    foods_array = [line.strip() for line in file]\n","\n","# Sample passage\n","# example = \"\"\"I enjoyed a scrumptious mushroom and leek strudel with phyllo dough, orange, mushroom and truffle risotto for breakfast. I also enjoyed the mushroom risottos with a little bit of mushroom sauce.I also had a great time with the roasted garlic and roasted red pepper flakes. The garlic was so good, I was able to get a good bite out of it. It was also very good with my husband's garlic bread.\"\"\"\n","\n","# Load the NER model\n","tokenizer = AutoTokenizer.from_pretrained(\"Dizex/FoodBaseBERT\")\n","model = AutoModelForTokenClassification.from_pretrained(\"Dizex/FoodBaseBERT\")\n","pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n","\n","# Get food entities using the NER model\n","ner_entity_results = pipe(example)\n","\n","# Create a list to store filtered food entities from the NER results\n","filtered_food_entities_ner = []\n","for result in ner_entity_results:\n","    entity = result[\"word\"]\n","    if entity.lower() in foods_array:\n","        filtered_food_entities_ner.append(entity.lower())\n","\n","# Function to extract food entities from text using the foods_array\n","def extract_food_entities(text):\n","    tokens = word_tokenize(text.lower())\n","    food_entities = []\n","    for food in foods_array:\n","        food_tokens = word_tokenize(food)\n","        if all(token in tokens for token in food_tokens):\n","            food_entities.append(food)\n","    return food_entities\n","\n","# Get food entities using nltk approach\n","filtered_food_entities_nltk = extract_food_entities(example)\n","\n","# Combine both results to get the final list of food entities\n","final_food_entities = list(set(filtered_food_entities_ner + filtered_food_entities_nltk))\n","\n","print(\"Food entities from NER model:\")\n","print(filtered_food_entities_ner)\n","print(len(filtered_food_entities_ner))\n","\n","print(\"\\nFood entities from nltk approach:\")\n","print(filtered_food_entities_nltk)\n","print(len(filtered_food_entities_nltk))\n","\n","print(\"\\nFinal list of food entities:\")\n","print(final_food_entities)\n","print(len(final_food_entities))"],"metadata":{"id":"O7JhVVhQIOWP","executionInfo":{"status":"aborted","timestamp":1700068958053,"user_tz":-330,"elapsed":2,"user":{"displayName":"nilupa illangarathna","userId":"05099327797442594291"}}},"execution_count":null,"outputs":[]}]}